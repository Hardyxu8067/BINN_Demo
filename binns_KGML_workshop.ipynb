{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biogeochemistry-Informed Neural Network (BINN)\n",
    "Adapted from the [BINN GitHub repository](https://github.com/Hardyxu8067/BINN) and the accompanying manuscript: [arXiv:2502.00672](https://arxiv.org/abs/2502.00672).\n",
    "#####  \n",
    "**Goal of this demo**  \n",
    "Learn how to (1) use BINN to improve a process‑based model’s predictive skill and (2) interpret the learned parameters for scientific insight.  \n",
    "#####  \n",
    "**Recall of BINN Structure**  \n",
    "BINN embeds a process‑based model inside a deep‑learning framework so its parameterization can be optimized from data. In this demo, the embedded process model is the soil‑organic‑carbon (SOC) module from CLM5 (Community Land Model v5, UCAR/NCAR). Here we provide a vectorized PyTorch re‑implementation of the matrix form of CLM5: <span style=\"color:DarkCyan\">*./fun_matrix_clm5_vectorized.py*</span>  \n",
    "To apply BINN in other domains, replace this CLM5 model with your target process-based model and provide the corresponding inputs/data. \n",
    "\n",
    "## Required Environment & dependencies\n",
    "The key packages to install are PyTorch, Numpy, Scipy, Pandas, matplotlib, scikit-learn, and cartopy.  \n",
    "You are recommended to use Google Colab to access this notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hardyxu8067/BINN_Demo/blob/master/binns_KGML_workshop.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Import the required libraries\n",
    "#-------------------------------\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "# Set HDF5_DISABLE_VERSION_CHECK to suppress version mismatch error\n",
    "import os\n",
    "os.environ['HDF5_DISABLE_VERSION_CHECK'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "#------Comment out this part if not using Colab------#\n",
    "# !git clone https://github.com/Hardyxu8067/BINN_Demo\n",
    "# import sys\n",
    "# os.chdir('/content/BINN_Demo')\n",
    "# sys.path.append('/content/BINN_Demo')\n",
    "#----------------------------------------------------#\n",
    "\n",
    "# Import Vectorized Versions of CLM5\n",
    "from fun_matrix_clm5_vectorized import fun_model_simu, fun_model_prediction\n",
    "from fun_matrix_clm5_vectorized_bulk_converge import fun_bulk_simu\n",
    "\n",
    "print(datetime.now(), '------------all packages loaded------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to edit here\n",
    "Use this cell to name your experiment, point to your data/output folders, and set training hyperparameters:  \n",
    "#####  \n",
    "**model_name** — the name of the process-based model.  \n",
    "**data_dir_input** — folder where your input data live.  \n",
    "**data_dir_output** — folder where results (checkpoints, logs, figures) will be saved.  \n",
    "**n_samples** — number of samples to use for this demo (which will affect the run time).  \n",
    "**Training hyperparameters**: val_split_ratio, test_split_ratio, emb_dim, lr, batch_size, num_epoch, weight_decay, scheduler_step_size, scheduler_gamma, patience.  \n",
    "**seed** — random seed for (approximate) reproducibility.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Input Hyperparameters\n",
    "################################################\n",
    "model_name = 'cesm2_clm5_cen_vr_v2'\n",
    "data_dir_input = './Input_Data/'\n",
    "data_dir_output = './Output_Data/'\n",
    "\n",
    "# NN model parameters\n",
    "n_samples = 200\n",
    "val_split_ratio = 0.1\n",
    "test_split_ratio = 0.1\n",
    "emb_dim = 50\n",
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "num_epoch = 50\n",
    "weight_decay = 0\n",
    "scheduler_step_size = 1000\n",
    "scheduler_gamma = 0\n",
    "patience = 500\n",
    "seed = 111\n",
    "\n",
    "# Set random seeds to try to ensure reproducibility\n",
    "def set_seeds(seed):\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\tif torch.cuda.is_available():\n",
    "\t\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = True\n",
    "set_seeds(seed)\n",
    "\n",
    "# Define the job ID\n",
    "job_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "job_id += (\"_\" + model_name)\n",
    "print(f\"New job ID: {job_id}\")\n",
    "\n",
    "# Define the device\n",
    "# Here CPU is enough\n",
    "device = \"cpu\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(data_dir_output, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir_output, job_id), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLM5 constants, parameters, forcing data, and other input data. \n",
    "This cell defines:  \n",
    "**(A)** CLM5 constants  \n",
    "**(B)** CLM5 trainable parameters  \n",
    "**(C)** the observational and forcing data that BINN loads for training. For this demo, all inputs are a subset of sites from New York State, extracted from the larger global datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# CLM5 constants\n",
    "#-------------------------------\n",
    "month_num = 12 \n",
    "soil_cpool_num = 7\n",
    "soil_decom_num = 20\n",
    "\n",
    "# parameters names\n",
    "para_names = ['diffus', 'cryo', 'q10', 'efolding', \\\n",
    "\t\t\t  'taucwd', 'taul1', 'taul2', 'tau4s1', 'tau4s2', 'tau4s3', \\\n",
    "\t\t\t  'fl1s1', 'fl2s1', 'fl3s2', 'fs1s2', 'fs1s3', 'fs2s1', 'fs2s3', 'fs3s1', 'fcwdl2', \\\n",
    "\t\t\t  'w-scaling', 'beta']\n",
    "para_index = np.arange(len(para_names))\n",
    "\n",
    "# soil depths info\n",
    "# width between two interfaces\n",
    "dz = np.array([2.000000000000000E-002, 4.000000000000000E-002, 6.000000000000000E-002, \\\n",
    "8.000000000000000E-002, 0.120000000000000, 0.160000000000000, \\\n",
    "0.200000000000000, 0.240000000000000, 0.280000000000000, \\\n",
    "0.320000000000000, 0.360000000000000, 0.400000000000000, \\\n",
    "0.440000000000000, 0.540000000000000, 0.640000000000000, \\\n",
    "0.740000000000000, 0.840000000000000, 0.940000000000000, \\\n",
    "1.04000000000000, 1.14000000000000, 2.39000000000000, \\\n",
    "4.67553390593274, 7.63519052838329, 11.1400000000000, \\\n",
    "15.1154248593737])\n",
    "\n",
    "# depth of the interface\n",
    "zisoi = np.array([2.000000000000000E-002, 6.000000000000000E-002, \\\n",
    "0.120000000000000, 0.200000000000000, 0.320000000000000, \\\n",
    "0.480000000000000, 0.680000000000000, 0.920000000000000, \\\n",
    "1.20000000000000, 1.52000000000000, 1.88000000000000, \\\n",
    "2.28000000000000, 2.72000000000000, 3.26000000000000, \\\n",
    "3.90000000000000, 4.64000000000000, 5.48000000000000, \\\n",
    "6.42000000000000, 7.46000000000000, 8.60000000000000, \\\n",
    "10.9900000000000, 15.6655339059327, 23.3007244343160, \\\n",
    "34.4407244343160, 49.5561492936897])\n",
    "\n",
    "zisoi_0 = 0\n",
    "\n",
    "# depth of the node\n",
    "zsoi = np.array([1.000000000000000E-002, 4.000000000000000E-002, 9.000000000000000E-002, \\\n",
    "0.160000000000000, 0.260000000000000, 0.400000000000000, \\\n",
    "0.580000000000000, 0.800000000000000, 1.06000000000000, \\\n",
    "1.36000000000000, 1.70000000000000, 2.08000000000000, \\\n",
    "2.50000000000000, 2.99000000000000, 3.58000000000000, \\\n",
    "4.27000000000000, 5.06000000000000, 5.95000000000000, \\\n",
    "6.94000000000000, 8.03000000000000, 9.79500000000000, \\\n",
    "13.3277669529664, 19.4831291701244, 28.8707244343160, \\\n",
    "41.9984368640029])\n",
    "\n",
    "# depth between two node\n",
    "dz_node = zsoi - np.append(np.array([0]), zsoi[:-1], axis = 0)\n",
    "\n",
    "# cesm2 resolution\n",
    "cesm2_resolution_lat = 180/384\n",
    "cesm2_resolution_lon = 360/576\n",
    "lon_grid = np.arange((-180 + cesm2_resolution_lon/2), 180, cesm2_resolution_lon)\n",
    "lat_grid = np.arange((90 - cesm2_resolution_lat/2), -90, -cesm2_resolution_lat)\n",
    "\n",
    "#-------------------------------\n",
    "# Load the data\n",
    "#-------------------------------\n",
    "# SOC observations\n",
    "profile_collection = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'profile_id.txt'), delimiter=',')\n",
    "original_profile_num = len(profile_collection)\n",
    "obs_soc_matrix = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'obs_soc_NY.txt'), delimiter=',')\n",
    "obs_soc_depth = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'obs_soc_depth.txt'), delimiter=',')\n",
    "profile_id = np.arange(len(profile_collection))\n",
    "print(original_profile_num, 'profiles loaded')\n",
    "\n",
    "# Environmental information at each site\n",
    "env_info = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'env_info.txt'), delimiter=',')\n",
    "col_max_min = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'col_max_min.txt'), delimiter=',')\n",
    "\n",
    "# Process-based model input data (forcing data) at each site\n",
    "# Carbon Input\n",
    "model_force_input_vector_cwd = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'model_force_input_vector_cwd.txt'), delimiter=',')\n",
    "model_force_input_vector_litter1 = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'model_force_input_vector_litter1.txt'), delimiter=',')\n",
    "model_force_input_vector_litter2 = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'model_force_input_vector_litter2.txt'), delimiter=',')\n",
    "model_force_input_vector_litter3 = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'model_force_input_vector_litter3.txt'), delimiter=',')\n",
    "# Max Active Layer Depth\n",
    "model_force_altmax_lastyear_profile = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'model_force_altmax_lastyear_profile.txt'), delimiter=',')\n",
    "model_force_altmax_current_profile = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'model_force_altmax_current_profile.txt'), delimiter=',')\n",
    "# Bedrock Layer\n",
    "model_force_nbedrock = np.loadtxt(os.path.join(data_dir_input, 'Training_Data', 'model_force_nbedrock.txt'), delimiter=',')\n",
    "# Oxygen Scalar\n",
    "model_force_xio = np.load(os.path.join(data_dir_input, 'Training_Data', 'model_force_xio.npy'), allow_pickle=True)\n",
    "# Nitrogen Scalar\n",
    "model_force_xin = np.load(os.path.join(data_dir_input, 'Training_Data', 'model_force_xin.npy'), allow_pickle=True)\n",
    "# Sand Content\n",
    "model_force_sand_vector = np.load(os.path.join(data_dir_input, 'Training_Data', 'model_force_sand_vector.npy'), allow_pickle=True)\n",
    "# Soil Temperature\n",
    "model_force_soil_temp_profile = np.load(os.path.join(data_dir_input, 'Training_Data', 'model_force_soil_temp_profile.npy'), allow_pickle=True)\n",
    "# Soil Moisture\n",
    "model_force_soil_water_profile = np.load(os.path.join(data_dir_input, 'Training_Data', 'model_force_soil_water_profile.npy'), allow_pickle=True)\n",
    "\n",
    "# Randomly select a subset of n_samples profiles for testing\n",
    "sample_profile_id = profile_id[np.random.choice(len(profile_id), n_samples, replace=False)]\n",
    "profile_collection = profile_collection[sample_profile_id]\n",
    "profile_id = profile_id[sample_profile_id]\n",
    "obs_soc_matrix = obs_soc_matrix[sample_profile_id, :]\n",
    "obs_soc_depth = obs_soc_depth[sample_profile_id, :]\n",
    "env_info = env_info[sample_profile_id, :]\n",
    "model_force_input_vector_cwd = model_force_input_vector_cwd[sample_profile_id, :]\n",
    "model_force_input_vector_litter1 = model_force_input_vector_litter1[sample_profile_id, :]\n",
    "model_force_input_vector_litter2 = model_force_input_vector_litter2[sample_profile_id, :]\n",
    "model_force_input_vector_litter3 = model_force_input_vector_litter3[sample_profile_id, :]\n",
    "model_force_altmax_lastyear_profile = model_force_altmax_lastyear_profile[sample_profile_id, :]\n",
    "model_force_altmax_current_profile = model_force_altmax_current_profile[sample_profile_id, :]\n",
    "model_force_nbedrock = model_force_nbedrock[sample_profile_id, :]\n",
    "model_force_xio = model_force_xio[sample_profile_id, :]\n",
    "model_force_xin = model_force_xin[sample_profile_id, :]\n",
    "model_force_sand_vector = model_force_sand_vector[sample_profile_id, :]\n",
    "model_force_soil_temp_profile = model_force_soil_temp_profile[sample_profile_id, :]\n",
    "model_force_soil_water_profile = model_force_soil_water_profile[sample_profile_id, :]\n",
    "\n",
    "print(datetime.now(), '------------all input data loaded with profile number:', len(profile_collection), '------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental Inputs and Prediction Data\n",
    "\n",
    "This cell scales continuous environmental variables to [0,1] using `col_max_min` (leaving categorical variables untransformed and preserving original `Lon`/`Lat`).  \n",
    "Then, the New York–state gridded prediction inputs are loaded (environmental covariates, CLM5 forcing data).  \n",
    "These data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------\n",
    "# Normalize env info (Input data)\n",
    "#---------------------------------------------------\n",
    "# environmental info names\n",
    "var4nn = ['Lon', 'Lat', \\\n",
    "'ESA_Land_Cover', \\\n",
    "'BIO1', 'BIO2', 'BIO3', 'BIO4', 'BIO12', 'BIO15', \\\n",
    "'Bulk_Density_0cm', 'Bulk_Density_30cm', 'Bulk_Density_100cm',\\\n",
    "'CEC_0cm', 'CEC_30cm', 'CEC_100cm', \\\n",
    "'Clay_Content_0cm', 'Clay_Content_30cm', 'Clay_Content_100cm', \\\n",
    "'Coarse_Fragments_v_0cm', 'Coarse_Fragments_v_30cm', 'Coarse_Fragments_v_100cm', \\\n",
    "'pH_Water_0cm', 'pH_Water_30cm', 'pH_Water_100cm', \\\n",
    "'Silt_Content_0cm', 'Silt_Content_30cm', 'Silt_Content_100cm', \\\n",
    "'SWC_v_Wilting_Point_0cm', 'SWC_v_Wilting_Point_30cm', 'SWC_v_Wilting_Point_100cm', \\\n",
    "'Texture_USDA_0cm', 'Texture_USDA_30cm', 'Texture_USDA_100cm', \\\n",
    "'USDA_Suborder', \\\n",
    "'WRB_Subgroup', \\\n",
    "'Elevation', \\\n",
    "'Koppen_Climate_2018', \\\n",
    "'nbedrock']\n",
    "# Categorical variables\n",
    "categorical_vars = [['ESA_Land_Cover'], ['Texture_USDA_0cm', 'Texture_USDA_30cm', 'Texture_USDA_100cm'], \n",
    "\t\t\t\t\t['USDA_Suborder'], ['WRB_Subgroup']]  # Variables inside a sub-list share the same categories\n",
    "categorical_vars_flattened = [item for sublist in categorical_vars for item in sublist]\n",
    "\n",
    "# Store the original longitude and latitude values\n",
    "original_lons = env_info[:, 0].copy()\n",
    "original_lats = env_info[:, 1].copy()\n",
    "\n",
    "# Don't want to transform categorical variables, so set max/min to nan\n",
    "for group in categorical_vars:\n",
    "\tfor var in group:\n",
    "\t\tidx = var4nn.index(var)\n",
    "\t\tcol_max_min[idx, :] = np.nan\n",
    "\n",
    "# Normalize the environmental information\n",
    "# Skip the longitude and latitude columns\n",
    "for ivar in np.arange(2, len(col_max_min[:, 0])):\n",
    "\tif np.isnan(col_max_min[ivar, :]).any():\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\tenv_info[:, ivar] = (env_info[:, ivar] - col_max_min[ivar, 0])/(col_max_min[ivar, 1] - col_max_min[ivar, 0])\n",
    "\t\tenv_info[(env_info[:, ivar] > 1), ivar] = 1\n",
    "\t\tenv_info[(env_info[:, ivar] < 0), ivar] = 0\n",
    "\n",
    "env_info = df(env_info)\n",
    "env_info.columns = var4nn\n",
    "\n",
    "print(datetime.now(), '------------Finish processing input data------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------\n",
    "# Load Data for Prediction\n",
    "#--------------------------------------------------- \n",
    "grid_env_info_ID = np.loadtxt(os.path.join(data_dir_input, 'Prediction_Data', 'grid_env_info_ID.txt'), delimiter=',')\n",
    "grid_env_info_NY = np.loadtxt(os.path.join(data_dir_input, 'Prediction_Data', 'grid_env_info_NY.txt'), delimiter=',')\n",
    "model_force_pred_input_vector_cwd = np.loadtxt(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_input_vector_cwd.txt'), delimiter=',')\n",
    "model_force_pred_input_vector_litter1 = np.loadtxt(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_input_vector_litter1.txt'), delimiter=',')\n",
    "model_force_pred_input_vector_litter2 = np.loadtxt(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_input_vector_litter2.txt'), delimiter=',')\n",
    "model_force_pred_input_vector_litter3 = np.loadtxt(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_input_vector_litter3.txt'), delimiter=',')\n",
    "model_force_pred_altmax_lastyear_profile = np.loadtxt(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_altmax_lastyear_profile.txt'), delimiter=',')\n",
    "model_force_pred_altmax_current_profile = np.loadtxt(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_altmax_current_profile.txt'), delimiter=',')\n",
    "model_force_pred_nbedrock = np.loadtxt(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_nbedrock.txt'), delimiter=',')\n",
    "model_force_pred_xio = np.load(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_xio.npy'), allow_pickle=True)\n",
    "model_force_pred_xin = np.load(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_xin.npy'), allow_pickle=True)\n",
    "model_force_pred_sand_vector = np.load(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_sand_vector.npy'), allow_pickle=True)\n",
    "model_force_pred_soil_temp_profile = np.load(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_soil_temp_profile.npy'), allow_pickle=True)\n",
    "model_force_pred_soil_water_profile = np.load(os.path.join(data_dir_input, 'Prediction_Data', 'model_force_pred_soil_water_profile.npy'), allow_pickle=True)\n",
    "\n",
    "# Normalize the environmental information\n",
    "# Skip the longitude and latitude columns\n",
    "for ivar in np.arange(2, len(col_max_min[:, 0])):\n",
    "\tif np.isnan(col_max_min[ivar, :]).any():\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\tgrid_env_info_NY[:, ivar] = (grid_env_info_NY[:, ivar] - col_max_min[ivar, 0])/(col_max_min[ivar, 1] - col_max_min[ivar, 0])\n",
    "\t\tgrid_env_info_NY[(grid_env_info_NY[:, ivar] > 1), ivar] = 1\n",
    "\t\tgrid_env_info_NY[(grid_env_info_NY[:, ivar] < 0), ivar] = 0\n",
    "\t\t\n",
    "grid_env_info_NY = df(grid_env_info_NY)\n",
    "grid_env_info_NY.columns = var4nn\n",
    "grid_lon = grid_env_info_NY['Lon'].values\n",
    "grid_lat = grid_env_info_NY['Lat'].values\n",
    "\n",
    "# Check the max and min value of categorical variables, if it is larger than the number of categories, then remove the row\n",
    "for group in categorical_vars:\n",
    "\tmask_max = grid_env_info_NY[group].apply(lambda x: (x > np.max(env_info[group])).any(), axis=1)\n",
    "\tmask_min = grid_env_info_NY[group].apply(lambda x: (x < np.min(env_info[group])).any(), axis=1)\n",
    "\tmask = mask_max | mask_min\n",
    "\tindices_to_remove = grid_env_info_NY[mask].index\n",
    "\tgrid_env_info_NY = grid_env_info_NY.drop(indices_to_remove)\n",
    "\tgrid_env_info_ID = grid_env_info_ID[~mask]\n",
    "\tmodel_force_pred_input_vector_cwd = model_force_pred_input_vector_cwd[~mask, :]\n",
    "\tmodel_force_pred_input_vector_litter1 = model_force_pred_input_vector_litter1[~mask, :]\n",
    "\tmodel_force_pred_input_vector_litter2 = model_force_pred_input_vector_litter2[~mask, :]\n",
    "\tmodel_force_pred_input_vector_litter3 = model_force_pred_input_vector_litter3[~mask, :]\n",
    "\tmodel_force_pred_altmax_lastyear_profile = model_force_pred_altmax_lastyear_profile[~mask, :]\n",
    "\tmodel_force_pred_altmax_current_profile = model_force_pred_altmax_current_profile[~mask, :]\n",
    "\tmodel_force_pred_nbedrock = model_force_pred_nbedrock[~mask, :]\t\n",
    "\tmodel_force_pred_xio = model_force_pred_xio[~mask]\n",
    "\tmodel_force_pred_xin = model_force_pred_xin[~mask]\n",
    "\tmodel_force_pred_sand_vector = model_force_pred_sand_vector[~mask]\n",
    "\tmodel_force_pred_soil_temp_profile = model_force_pred_soil_temp_profile[~mask]\n",
    "\tmodel_force_pred_soil_water_profile = model_force_pred_soil_water_profile[~mask]\n",
    "\tgrid_lon = grid_lon[~mask]\n",
    "\tgrid_lat = grid_lat[~mask]\n",
    "\n",
    "\n",
    "print(f'Number of grid points in NY State: {len(grid_env_info_NY)}')\n",
    "print('-----------------------Finish processing prediction data-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------\n",
    "# Organize training data\n",
    "#---------------------------------------------------\n",
    "# Remove the 'Lon' and 'Lat' columns from var4nn\n",
    "var4nn = [var for var in var4nn if var not in ['Lon', 'Lat']] \n",
    "\n",
    "# Environmental information and Model forcing data\n",
    "current_data_x = np.ones((len(profile_collection),  max(len(var4nn), 20), 12, 13))*np.nan\n",
    "current_data_x[:, 0:len(var4nn), 0, 0] = env_info[var4nn]\n",
    "current_data_x[:, 0:12, 0, 1] = model_force_input_vector_cwd\n",
    "current_data_x[:, 0:12, 0, 2] = model_force_input_vector_litter1\n",
    "current_data_x[:, 0:12, 0, 3] = model_force_input_vector_litter2\n",
    "current_data_x[:, 0:12, 0, 4] = model_force_input_vector_litter3\n",
    "current_data_x[:, 0:12, 0, 5] = model_force_altmax_lastyear_profile\n",
    "current_data_x[:, 0:12, 0, 6] = model_force_altmax_current_profile\n",
    "current_data_x[:, 0:12, 0, 7] = model_force_nbedrock\n",
    "\n",
    "current_data_x[:, 0:20, 0:12, 8] = model_force_xio\n",
    "current_data_x[:, 0:20, 0:12, 9] = model_force_xin\n",
    "current_data_x[:, 0:20, 0:12, 10] = model_force_sand_vector\n",
    "current_data_x[:, 0:20, 0:12, 11] = model_force_soil_temp_profile\n",
    "current_data_x[:, 0:20, 0:12, 12] = model_force_soil_water_profile\n",
    "\n",
    "# Include SOC data in current_data_y\n",
    "current_data_y = obs_soc_matrix\n",
    "\n",
    "# Include SOC depth in current_data_z\n",
    "current_data_z = obs_soc_depth\n",
    "\n",
    "# Check if there're any negative values in the data\n",
    "print(\"Negative values in current_data_y\", np.sum(current_data_y < 0))\n",
    "\n",
    "lons = original_lons\n",
    "lats = original_lats\n",
    "\n",
    "nan_loc = np.sum(current_data_x[:, 0:len(var4nn), 0, 0], axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_input_vector_cwd, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_input_vector_litter1, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_input_vector_litter2, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_input_vector_litter3, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_altmax_lastyear_profile, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_altmax_current_profile, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_nbedrock, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_xio, axis = (1, 2)) + \\\n",
    "\t\t\tnp.sum(model_force_xin, axis = (1, 2)) + \\\n",
    "\t\t\tnp.sum(model_force_sand_vector, axis = (1, 2)) + \\\n",
    "\t\t\tnp.sum(model_force_soil_temp_profile, axis = (1, 2)) + \\\n",
    "\t\t\tnp.sum(model_force_soil_water_profile, axis = (1, 2)) \n",
    "\n",
    "nan_mask = ~np.isnan(nan_loc)\n",
    "\n",
    "neg_mask = ~np.any(current_data_y < 0, axis=1)\n",
    "\n",
    "valid_profile_loc = np.where(nan_mask & neg_mask)[0]\n",
    "\n",
    "current_data_y = current_data_y[valid_profile_loc, :]\n",
    "current_data_z = current_data_z[valid_profile_loc, :]\n",
    "current_data_x = current_data_x[valid_profile_loc, :, :, :]\n",
    "current_profile_collection = profile_collection[valid_profile_loc]\n",
    "current_profile_id = profile_id[valid_profile_loc]\n",
    "lons = lons[valid_profile_loc]\n",
    "lats = lats[valid_profile_loc]\n",
    "\n",
    "print(\"Shape of current data x\", current_data_x.shape)\n",
    "print(\"Shape of current data y\", current_data_y.shape)\n",
    "print(\"Shape of current data z\", current_data_z.shape)\n",
    "print(\"Shape of current data profile_id\", current_profile_collection.shape)\n",
    "print(\"Shape of current data profile_id\", current_profile_id.shape)\n",
    "print(\"Shape of lons\", lons.shape)\n",
    "print(\"Shape of lats\", lats.shape)\n",
    "print(\"Negative values in current_data_y\", np.sum(current_data_y < 0))\n",
    "\n",
    "# Prediction data\n",
    "predict_data_x = np.ones((len(grid_env_info_ID), max(len(var4nn), 20), 12, 13))*np.nan\n",
    "predict_data_x[:, 0:len(var4nn), 0, 0] = grid_env_info_NY[var4nn]\n",
    "predict_data_x[:, 0:12, 0, 1] = model_force_pred_input_vector_cwd\n",
    "predict_data_x[:, 0:12, 0, 2] = model_force_pred_input_vector_litter1\n",
    "predict_data_x[:, 0:12, 0, 3] = model_force_pred_input_vector_litter2\n",
    "predict_data_x[:, 0:12, 0, 4] = model_force_pred_input_vector_litter3\n",
    "predict_data_x[:, 0:12, 0, 5] = model_force_pred_altmax_lastyear_profile\n",
    "predict_data_x[:, 0:12, 0, 6] = model_force_pred_altmax_current_profile\n",
    "predict_data_x[:, 0:12, 0, 7] = model_force_pred_nbedrock\n",
    "predict_data_x[:, 0:20, 0:12, 8] = model_force_pred_xio\n",
    "predict_data_x[:, 0:20, 0:12, 9] = model_force_pred_xin\n",
    "predict_data_x[:, 0:20, 0:12, 10] = model_force_pred_sand_vector\n",
    "predict_data_x[:, 0:20, 0:12, 11] = model_force_pred_soil_temp_profile\n",
    "predict_data_x[:, 0:20, 0:12, 12] = model_force_pred_soil_water_profile\n",
    "\n",
    "# Create a dummy depth array for prediction\n",
    "predict_data_z = np.zeros((len(grid_env_info_ID), 200))\n",
    "\n",
    "nan_loc_pred = np.sum(predict_data_x[:, 0:len(var4nn), 0, 0], axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_pred_input_vector_cwd, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_pred_input_vector_litter1, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_pred_input_vector_litter2, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_pred_input_vector_litter3, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_pred_altmax_lastyear_profile, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_pred_altmax_current_profile, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_pred_nbedrock, axis = 1) + \\\n",
    "\t\t\tnp.sum(model_force_pred_xio, axis = (1, 2)) + \\\n",
    "\t\t\tnp.sum(model_force_pred_xin, axis = (1, 2)) + \\\n",
    "\t\t\tnp.sum(model_force_pred_sand_vector, axis = (1, 2))\n",
    "nan_mask_pred = ~np.isnan(nan_loc_pred)\n",
    "\n",
    "predict_data_x = predict_data_x[nan_mask_pred, :, :, :]\n",
    "predict_data_z = predict_data_z[nan_mask_pred, :]\n",
    "grid_lat = grid_lat[nan_mask_pred]\n",
    "grid_lon = grid_lon[nan_mask_pred]\n",
    "\n",
    "\n",
    "## Train, validation, test split\n",
    "# Determine the number of training samples based on the ratios\n",
    "train_loc = np.random.choice(np.arange(0, len(current_data_x[:, 0])), size=round((1 - val_split_ratio - test_split_ratio) * len(current_data_x[:, 0])), replace=False)\n",
    "# The remaining data after removing the training samples\n",
    "remaining_loc = np.setdiff1d(np.arange(0, len(current_data_x[:, 0])), train_loc)\n",
    "# Split the remaining data into validation and test sets\n",
    "num_val_samples = round(val_split_ratio / (val_split_ratio + test_split_ratio) * len(remaining_loc))\n",
    "val_loc = np.random.choice(remaining_loc, size=num_val_samples, replace=False)\n",
    "test_loc = np.setdiff1d(remaining_loc, val_loc)\n",
    "\n",
    "train_y = torch.tensor(current_data_y[train_loc, :], dtype=torch.float32)\n",
    "val_y = torch.tensor(current_data_y[val_loc, :], dtype=torch.float32)\n",
    "test_y = torch.tensor(current_data_y[test_loc, :], dtype=torch.float32)\n",
    "\n",
    "train_z = torch.tensor(current_data_z[train_loc, :], dtype=torch.float32)\n",
    "val_z = torch.tensor(current_data_z[val_loc, :], dtype=torch.float32)\n",
    "test_z = torch.tensor(current_data_z[test_loc, :], dtype=torch.float32)\n",
    "\n",
    "train_x = torch.tensor(current_data_x[train_loc, :, :, :], dtype=torch.float32)\n",
    "val_x = torch.tensor(current_data_x[val_loc, :, :, :], dtype=torch.float32)\n",
    "test_x = torch.tensor(current_data_x[test_loc, :, :, :], dtype=torch.float32)\n",
    "\n",
    "train_profile_id = torch.tensor(current_profile_id[train_loc], dtype=torch.long)\n",
    "val_profile_id = torch.tensor(current_profile_id[val_loc], dtype=torch.long)\n",
    "test_profile_id = torch.tensor(current_profile_id[test_loc], dtype=torch.long)\n",
    "\n",
    "\n",
    "print(datetime.now(), '------------nn data prepared------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the map of the training data\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())      \n",
    "ax.set_global()                                  \n",
    "ax.add_feature(cfeature.LAND, facecolor='0.95', zorder=0)\n",
    "ax.add_feature(cfeature.OCEAN, facecolor='white', zorder=0)\n",
    "ax.coastlines(linewidth=0.8)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.6)\n",
    "ax.scatter(lons, lats,\n",
    "           s=10, c='blue', alpha=0.8,\n",
    "           transform=ccrs.PlateCarree())\n",
    "margin = 1\n",
    "ax.set_extent([lons.min()-margin, lons.max()+margin,\n",
    "               lats.min()-margin, lats.max()+margin],\n",
    "              crs=ccrs.PlateCarree())\n",
    "gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.4)\n",
    "gl.right_labels = False\n",
    "gl.top_labels = False\n",
    "plt.title('Training Data Locations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of SOC data along the soil depths\n",
    "While CLM5 simulates SOC dynamics at 20 specific depths, SOC data collected from the field were not necessarily measured at the depth nodes set in CLM5 simulation.  \n",
    "**(A)** Thus, in calculating the loss function value, for observations at depths equaling CLM5 nodes, the simulated values were directly from CLM5 outputs.  \n",
    "**(B)** When observations occur at depths between two CLM5 nodes, we employed linear interpolation to estimate simulated SOC values at the observation depths.  \n",
    "**(C)** In cases where observations extend beyond 8 meters (i.e., the deepest node in CLM5 simulations), we used the values at 8 meters as simulated SOC as SOC concentration in deeper layers no longer changes much.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Simulation of SOC looks like\n",
    "iprofile = 3\n",
    "# assume parameters all at the mean value (0.5 for all 21 parameters)\n",
    "temp_para = torch.tensor([0.5] * 21, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "simu_soc_temp = fun_model_prediction(temp_para, torch.tensor(current_data_x[iprofile, :, :, :]).unsqueeze(0), soil_decom_num)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# SOC Simulation\n",
    "plt.scatter(simu_soc_temp[:, 0:20], zsoi[0:20], s=20, alpha=1, color='red')\n",
    "# SOC Observations\n",
    "plt.scatter(obs_soc_matrix[iprofile,], obs_soc_depth[iprofile,], s=20, alpha=1, color='blue')\n",
    "plt.title('Distribution of SOC')\n",
    "plt.xlabel('SOC (g C m$^{-3}$)')\n",
    "plt.ylabel('Depth (m)')\n",
    "plt.ylim(0, 2)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend(['Model Simulation', 'SOC Observations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and BINN Structure\n",
    "**Loss Function**: Because we are interested in accurately simulating SOC, our primary loss uses Smooth L1/L1/L2 loss to quantify the discrepancy between simulated and observed SOC values. The loss function also includes an additional hyperbolic cosine loss (cosh) term that acts as a regulator, encouraging the neural network to predict biogeochemical parameters within reasonable bounds.  \n",
    "**BINN Structure**: First, a neural network learns the relationships between environmental covariates and biogeochemical parameters, which quantify the strength of important processes in the soil carbon cycle. We pass these parameters to a process-based model to simulate SOC storage, and compare with field observations.  \n",
    "**NN Structure**: The neural network component processes categorical environmental covariates through an embedding layer and the network outputs are transformed via a sigmoid activation to generate 21 biogeochemical parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the cosh function look like?\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.cosh(x)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label='cosh(x)')\n",
    "plt.title('Cosh Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('cosh(x)')\n",
    "plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binns_loss(y_pred, y_true, pred_para):\n",
    "    # process modeling\n",
    "\tsoc_simu = y_pred\n",
    "\t# observations\n",
    "\tsoc_true = y_true\n",
    "\t# predicted parameters\n",
    "\tpred_para = pred_para\n",
    "\n",
    "\t# flatten simulation\n",
    "\tsoc_simu_vector = torch.reshape(soc_simu, [1, -1])\n",
    "\tsoc_true_vector = torch.reshape(soc_true, [1, -1])\n",
    "\n",
    "\t# exclude nan\n",
    "\tvalid_loc = torch.where(torch.isnan(soc_simu_vector+soc_true_vector) == False)\n",
    "\tsoc_simu_vector = soc_simu_vector[valid_loc]\n",
    "\tsoc_true_vector = soc_true_vector[valid_loc]\n",
    "\n",
    "\t# modeling inefficiency\n",
    "\tmodeling_inefficiency_soc = 1- torch.sum((soc_simu_vector - soc_true_vector)**2)/torch.sum((soc_true_vector - torch.mean(soc_true_vector))**2)\n",
    "\n",
    "\t# Regularization for predicted parameters using cosh\n",
    "\t# Encourage parameters to be around 0.5\n",
    "\ttarget_value = 0.5\n",
    "\tscale_factor = 100\n",
    "\tparameter_regularization_loss = torch.mean(torch.cosh(scale_factor*(pred_para - target_value)) - 1)\n",
    "\n",
    "\t# Weighting factor for regularization term\n",
    "\tregularization_weight = 100\n",
    "\n",
    "\t# Calculate the loss\n",
    "\t# ** You may try different loss functions here **\n",
    "\t# Smooth l1 loss\n",
    "\t# l1_loss = torch.nn.functional.smooth_l1_loss(soc_simu_vector, soc_true_vector, reduction='mean')\n",
    "\t# l1 loss\n",
    "\t# l1_loss = torch.nn.functional.l1_loss(soc_simu_vector, soc_true_vector, reduction='mean')\n",
    "\t# l2 loss\n",
    "\tl1_loss = torch.nn.functional.mse_loss(soc_simu_vector, soc_true_vector, reduction='mean')\n",
    "\t\n",
    "\tloss = l1_loss + regularization_weight * parameter_regularization_loss\n",
    "\n",
    "\treturn loss, modeling_inefficiency_soc, l1_loss\n",
    "\n",
    "class nn_model(nn.Module):\n",
    "\tdef __init__(self, var_idx_to_emb):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# Dict from categorical variable index -> Embedding layer we use\n",
    "\t\tself.var_idx_to_emb = var_idx_to_emb\n",
    "\n",
    "\t\t# List of non-categorical variable indices\n",
    "\t\tself.non_categorical_indices = list(set(list(range(len(var4nn)))).difference(var_idx_to_emb.keys()))\n",
    "\t\tnew_input_size = len(self.non_categorical_indices)\n",
    "\t\tfor idx, emb in self.var_idx_to_emb.items():\n",
    "\t\t\tnew_input_size += emb.embedding_dim\n",
    "\n",
    "\t\t# Neural network layers\n",
    "\t\t# first layer\n",
    "\t\tself.l1 = nn.Linear(new_input_size, 128)\n",
    "\t\t\n",
    "\t\t# second layer\n",
    "\t\tself.l2 = nn.Linear(128, 128)\n",
    "\n",
    "\t\t# third layer\n",
    "\t\tself.l3 = nn.Linear(128, 128)\n",
    "\n",
    "\t\t# fourth layer\n",
    "\t\tself.l4 = nn.Linear(128, len(para_index))\n",
    "\n",
    "\t\t# Dropout layers\n",
    "\t\tself.dropout = nn.Dropout(0)\n",
    "\n",
    "\t\t# leaky relu\n",
    "\t\tself.leaky_relu = nn.LeakyReLU(negative_slope=0.3)\n",
    "\n",
    "\t\t# sigmoid parameters\n",
    "\t\tself.temp_sigmoid = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "\t\t# sigmoid\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\t\t# batch normalization\n",
    "\t\tself.bn1 = nn.BatchNorm1d(128)\n",
    "\t\tself.bn2 = nn.BatchNorm1d(128)\n",
    "\t\tself.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "\t\t# Initialize weights\n",
    "\t\tgain_leaky_relu = nn.init.calculate_gain('leaky_relu', 0.3)\n",
    "\t\tgain_sigmoid = nn.init.calculate_gain('sigmoid')\n",
    "\t\tnn.init.xavier_uniform_(self.l1.weight, gain=gain_leaky_relu)\n",
    "\t\tnn.init.xavier_uniform_(self.l2.weight, gain=gain_leaky_relu)\n",
    "\t\tnn.init.xavier_uniform_(self.l3.weight, gain=gain_leaky_relu)\n",
    "\t\tnn.init.xavier_uniform_(self.l4.weight, gain=gain_sigmoid)\n",
    "\n",
    "\t\t# Initialize biases\n",
    "\t\tnn.init.zeros_(self.l1.bias)\n",
    "\t\tnn.init.zeros_(self.l2.bias)\n",
    "\t\tnn.init.zeros_(self.l3.bias)\n",
    "\t\tnn.init.zeros_(self.l4.bias)\n",
    "\n",
    "\tdef forward(self, input_var, wosis_depth, soil_layer= soil_decom_num, whether_predict = 0):\n",
    "\t\tpredictor = input_var[:, :, 0, 0]\n",
    "\t\tforcing = input_var[:, :, :, :]\n",
    "\t\tobs_depth = wosis_depth\n",
    "\n",
    "\t\t# Compute embeddings for all categorical variables\n",
    "\t\tembs = []\n",
    "\t\tfor idx, embedding_layer in self.var_idx_to_emb.items():\n",
    "\t\t\t## embedding layer ##\n",
    "\t\t\temb = embedding_layer(predictor[:, idx].int())\n",
    "\t\t\temb = F.normalize(emb, p=2, dim=1) # Normalize embeddings\n",
    "\t\t\tembs.append(emb)\n",
    "\t\tall_embs = torch.concatenate(embs, dim=1)\n",
    "\n",
    "\t\tnew_input = torch.concatenate([predictor[:, self.non_categorical_indices], all_embs], dim=1)\n",
    "\n",
    "\t\t# hidden layers\n",
    "\t\t# layer 1\n",
    "\t\th1 = self.l1(new_input)\n",
    "\t\th1 = self.bn1(h1)\n",
    "\t\th1 = self.leaky_relu(h1)\n",
    "\t\th1 = self.dropout(h1)\n",
    "\t\t# layer 2\n",
    "\t\th2 = self.l2(h1)\n",
    "\t\th2 = self.bn2(h2)\n",
    "\t\th2 = self.leaky_relu(h2) # + h1 # residual connection\n",
    "\t\th2 = self.dropout(h2)\n",
    "\t\t# layer 3\n",
    "\t\th3 = self.l3(h2) \n",
    "\t\th3 = self.bn3(h3)\n",
    "\t\th3 = self.leaky_relu(h3) \n",
    "\t\th3 = self.dropout(h3)\n",
    "\t\t# Clamp temp_sigmoid\n",
    "\t\tclamped_temp_sigmoid = 10 + 99 * self.sigmoid(self.temp_sigmoid) \n",
    "\t\th5 = self.sigmoid(self.l4(h3)/clamped_temp_sigmoid)\n",
    "\t\t\n",
    "\t\tif whether_predict == 0:\n",
    "\t\t\tsimu_soc = fun_model_simu(h5, forcing, obs_depth, soil_layer)\n",
    "\t\telse: \n",
    "\t\t\tsimu_soc = fun_model_prediction(h5, forcing, soil_layer)\n",
    "\t\t\n",
    "\t\treturn simu_soc, h5\n",
    "\n",
    "# Helper function to combine the training data into a single tensor\n",
    "class MergeDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y, data_z, profile_id):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.data_z = data_z\n",
    "        self.profile_id = profile_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx], self.data_y[idx], self.data_z[idx], self.profile_id[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for categorical variables (each int maps to a different category)\n",
    "var_idx_to_emb = dict()  # Column index to Embedding layer to use\n",
    "for group in categorical_vars:\n",
    "\tn_categories = int(np.nanmax(env_info[group]) + 1)\n",
    "\t# n_categories = int(max(np.nanmax(env_info[group]), np.nanmax(grid_env_info_NY[group])) + 1)\n",
    "\temb = nn.Embedding(num_embeddings=n_categories, embedding_dim=emb_dim).to(device)\n",
    "\tfor var in group:\n",
    "\t\tidx = var4nn.index(var)\n",
    "\t\tvar_idx_to_emb[idx] = emb\n",
    "\n",
    "# Create the model\n",
    "model = nn_model(var_idx_to_emb).to(device)\n",
    "print(model)\n",
    "# Loss , scheduler, and optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "fun_loss = binns_loss\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = MergeDataset(train_x.to(device), train_y.to(device), train_z.to(device), train_profile_id.to(device))\n",
    "val_dataset = MergeDataset(val_x.to(device), val_y.to(device), val_z.to(device), val_profile_id.to(device))\n",
    "\n",
    "# Data loaders with DistributedSampler\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "global train_loss_history, train_NSE_history_soc, val_loss_history, val_NSE_history_soc\n",
    "\n",
    "# record the loss history\n",
    "train_loss_history = np.ones((num_epoch, 1))*np.nan\n",
    "train_NSE_history_soc = np.ones((num_epoch, 1))*np.nan\n",
    "val_loss_history = np.ones((num_epoch, 1))*np.nan\n",
    "val_NSE_history_soc = np.ones((num_epoch, 1))*np.nan\n",
    "best_model_epoch = torch.tensor(0) \n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf') \n",
    "best_val_NSE_soc = 0 \n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Define starting point\n",
    "start_epoch = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for iepoch in range(start_epoch, num_epoch):\n",
    "\t# Initialize tensors to store predictions\n",
    "\tval_pred_soc = torch.tensor(np.ones((original_profile_num, 200))*np.nan, dtype = torch.float32, device=device)\n",
    "\tval_pred_para = torch.tensor(np.ones((original_profile_num, len(para_names)))*np.nan, dtype = torch.float32, device=device)\n",
    "\t\n",
    "\ttest_pred_soc = torch.tensor(np.ones((original_profile_num, 200))*np.nan, dtype = torch.float32, device=device)\n",
    "\ttest_pred_para = torch.tensor(np.ones((original_profile_num, len(para_names)))*np.nan, dtype = torch.float32, device=device)\n",
    "\n",
    "\t# clear gradients\n",
    "\toptimizer.zero_grad()\n",
    "\tmodel.zero_grad()\n",
    "\n",
    "\t# Start training\n",
    "\ttotal_loss_record_train = list()\n",
    "\tl1_loss_record_train = list()\n",
    "\tNSE_soc_record_train = list()\n",
    "\tibatch = 0\n",
    "\tepoch_start = time.time()\n",
    "\tmodel.train()\n",
    "\n",
    "\n",
    "\tfor batch_info in train_loader:\n",
    "\t\tbatch_x, batch_y, batch_z, batch_profile_id = batch_info\n",
    "\t\tibatch = ibatch + 1\n",
    "\n",
    "\t\tbatch_x = batch_x.to(device)\n",
    "\t\tbatch_y = batch_y.to(device)\n",
    "\n",
    "\t\tbatch_y_hat, batch_pred_para = model(batch_x, batch_z, whether_predict = 0)\n",
    "\n",
    "\t\ttotal_loss, train_NSE_soc,smooth_l1_loss = fun_loss(batch_y_hat, batch_y, batch_pred_para)\n",
    "\n",
    "\t\tobj = total_loss\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tobj.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\ttotal_loss_record_train.append(total_loss.item())\n",
    "\t\tl1_loss_record_train.append(smooth_l1_loss.item())\n",
    "\t\tNSE_soc_record_train.append(train_NSE_soc.item())\n",
    "\n",
    "\t# training time\n",
    "\ttrain_time = time.time() - epoch_start\n",
    "\n",
    "\t# -------------------------------------validation\n",
    "\ttotal_loss_record_val = list()\n",
    "\tl1_loss_record_val = list()\n",
    "\tNSE_soc_record_val = list()\n",
    "\n",
    "\tibatch = 0\n",
    "\tmodel.eval()\n",
    "\n",
    "\tfor batch_info in val_loader:\n",
    "\t\tbatch_x, batch_y, batch_z, batch_profile_id = batch_info\n",
    "\n",
    "\t\tbatch_x = batch_x.to(device)\n",
    "\t\tbatch_y = batch_y.to(device)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tbatch_y_hat, batch_pred_para = model(batch_x, batch_z, whether_predict = 0)\n",
    "\n",
    "\t\tobj, val_NSE_soc, val_l1_loss = fun_loss(batch_y_hat, batch_y, batch_pred_para)\n",
    "\t\t\n",
    "\t\ttotal_loss_record_val.append(obj.item())\n",
    "\t\tl1_loss_record_val.append(val_l1_loss.item())\n",
    "\t\tNSE_soc_record_val.append(val_NSE_soc.item())\n",
    "\n",
    "\t\tibatch = ibatch + 1\n",
    "\n",
    "\t# end for batch_info in val_loader\n",
    "\t\t\n",
    "\t# record the time\n",
    "\thist_time = time.time() - start_time\n",
    "\n",
    "\t# record the loss history\n",
    "\ttrain_loss_history[iepoch, :] = torch.tensor(total_loss_record_train).mean().detach().cpu().numpy()\n",
    "\ttrain_NSE_history_soc[iepoch, :] = torch.tensor(NSE_soc_record_train).mean().detach().cpu().numpy()\n",
    "\tval_loss_history[iepoch, :] = torch.tensor(total_loss_record_val).mean().detach().cpu().numpy()\n",
    "\tval_NSE_history_soc[iepoch, :] = torch.tensor(NSE_soc_record_val).mean().detach().cpu().numpy()\n",
    "\n",
    "\tprint(f'Epoch {iepoch}, train SOC NSE: {torch.tensor(NSE_soc_record_train).mean():.2f}, train L1 loss: {torch.tensor(l1_loss_record_train).mean():.2f}, train time: {train_time:.2f}, \\\n",
    "\tvalidation SOC NSE: {torch.tensor(NSE_soc_record_val).mean():.2f}, validation L1 loss: {torch.tensor(l1_loss_record_val).mean():.2f}, hist time: {hist_time:.2f}', flush=True)\n",
    "\n",
    "\n",
    "\tif val_NSE_history_soc[iepoch, :] > best_val_NSE_soc:\n",
    "\t\tprint(f'Best model updated at epoch {iepoch}', flush=True)\n",
    "\t\t\n",
    "\t\tbest_model_epoch = torch.tensor(iepoch, device=device)\n",
    "\n",
    "\t\tcheckpoint_best_model = {\n",
    "\t\t\t'epoch': iepoch,\n",
    "\t\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t\t\t'best_val_loss': best_val_loss,\n",
    "\t\t\t'best_val_NSE_soc': best_val_NSE_soc,\n",
    "\t\t\t'best_model_epoch': best_model_epoch,\n",
    "\t\t\t'train_loss_history': train_loss_history,\n",
    "\t\t\t'train_NSE_history_soc': train_NSE_history_soc,\n",
    "\t\t\t'val_loss_history': val_loss_history,\n",
    "\t\t\t'val_NSE_history_soc': val_NSE_history_soc,\n",
    "\t\t\t'train_indices': train_loc,\n",
    "\t\t\t'val_indices': val_loc,\n",
    "\t\t\t'test_indices': test_loc,\n",
    "\t\t\t'epochs_without_improvement': epochs_without_improvement,\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\tbest_model_path = data_dir_output + job_id + '/opt_nn_' + job_id  + '.pt'\n",
    "\t\ttorch.save(checkpoint_best_model, best_model_path)\n",
    "\n",
    "\t\tbest_val_NSE_soc = val_NSE_history_soc[iepoch, :]\n",
    "\n",
    "print(\"-----------------Model Training Finished at \" + str(datetime.now()) + \"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# prediction bv best trained model\n",
    "##################################################\n",
    "new_checkpoint = torch.load(data_dir_output + job_id + '/opt_nn_' + job_id + '.pt', map_location=device, weights_only=False)\n",
    "best_guess_model = model  # Do not need to create a new model\n",
    "best_guess_model.load_state_dict(new_checkpoint['model_state_dict'])\n",
    "\n",
    "best_guess_model.eval()\n",
    "with torch.no_grad():\n",
    "\tbest_guess_val_y_hat, best_guess_val_pred_para = best_guess_model(val_x.to(device), val_z.to(device), whether_predict = 0)\n",
    "\tbest_guess_train_y_hat, best_guess_train_pred_para = best_guess_model(train_x.to(device), train_z.to(device), whether_predict = 0)\n",
    "\ttrain_loss, train_NSE_soc, train_l1_loss = fun_loss(best_guess_train_y_hat, train_y.to(device), best_guess_train_pred_para)\n",
    "\tval_loss, val_NSE_soc, val_l1_loss = fun_loss(best_guess_val_y_hat, val_y.to(device), best_guess_val_pred_para)\n",
    "\tprint(f'Train loss: {train_loss.item():.4f}, Train NSE_soc: {train_NSE_soc.item():.4f}, Train L1 loss: {train_l1_loss.item():.4f}')\n",
    "\tprint(f'Validation loss: {val_loss.item():.4f}, Validation NSE_soc: {val_NSE_soc.item():.4f}, Validation L1 loss: {val_l1_loss.item():.4f}')\n",
    "\tif test_split_ratio != 0:\n",
    "\t\tbest_guess_test_y_hat, best_guess_test_pred_para = best_guess_model(test_x.to(device), test_z.to(device), whether_predict = 0)\n",
    "\t\ttest_loss, test_NSE_soc, test_l1_loss = fun_loss(best_guess_test_y_hat, test_y.to(device), best_guess_test_pred_para)\n",
    "\t\tprint(f'Test loss: {test_loss.item():.4f}, Test NSE_soc: {test_NSE_soc.item():.4f}, Test L1 loss: {test_l1_loss.item():.4f}')\n",
    "\n",
    "print(\"-----------------Model Test Finished at \" + str(datetime.now()) + \"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation NSE history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_NSE_history_soc, label='Training Loss', color='orange')\n",
    "plt.plot(val_NSE_history_soc, label='Validation NSE', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss / NSE')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Training and Validation Loss / NSE History')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter plot of SOC simulation vs observation\n",
    "train_soc_simu = best_guess_train_y_hat[~torch.isnan(best_guess_train_y_hat)]\n",
    "train_soc_obs = train_y[~torch.isnan(train_y)]\n",
    "val_soc_simu = best_guess_val_y_hat[~torch.isnan(best_guess_val_y_hat)]\n",
    "val_soc_obs = val_y[~torch.isnan(val_y)]\n",
    "test_soc_simu = best_guess_test_y_hat[~torch.isnan(best_guess_test_y_hat)]\n",
    "test_soc_obs = test_y[~torch.isnan(test_y)]\n",
    "axis_max = max(train_soc_simu.max(), train_soc_obs.max())\n",
    "# Log transform the data\n",
    "train_soc_simu = np.log(train_soc_simu.cpu().numpy() + 1e-10)\n",
    "train_soc_obs = np.log(train_soc_obs.cpu().numpy() + 1e-10)\n",
    "val_soc_simu = np.log(val_soc_simu.cpu().numpy() + 1e-10)\n",
    "val_soc_obs = np.log(val_soc_obs.cpu().numpy() + 1e-10)\n",
    "test_soc_simu = np.log(test_soc_simu.cpu().numpy() + 1e-10)\n",
    "test_soc_obs = np.log(test_soc_obs.cpu().numpy() + 1e-10)\n",
    "axis_max = np.log(axis_max) + 5\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(train_soc_simu, train_soc_obs, s=20, alpha=0.5, color='orange', label='Training Data')\n",
    "plt.scatter(val_soc_simu, val_soc_obs, s=20, alpha=0.5, color='blue', label='Validation Data')\n",
    "plt.scatter(test_soc_simu, test_soc_obs, s=20, alpha=0.5, color='green', label='Test Data')\n",
    "plt.legend()\n",
    "plt.plot([0, axis_max], [0, axis_max], color='red', linestyle='--') \n",
    "plt.xlabel('SOC Simulation (log(g C m$^{-3}$))')\n",
    "plt.ylabel('SOC Observation (log(g C m$^{-3}$))')\n",
    "plt.title('SOC Simulation vs Observation')\n",
    "plt.xlim(0, axis_max)\n",
    "plt.ylim(0, axis_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of the Optimized Parameters\n",
    "Here we plot the distributions of CLM5 parameters predicted by BINN. Parameters that remain in their initialization value (~0.5) suggest low sensitivity of the SOC simulation to those parameters. By contrast, parameters that deviate from 0.5 —e.g., **q10**, **efolding**, **fs1s2**—appear more influential and better informed by the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution of predicted parameters\n",
    "train_para = best_guess_train_pred_para.cpu().numpy()\n",
    "val_para = best_guess_val_pred_para.cpu().numpy()\n",
    "test_para = best_guess_test_pred_para.cpu().numpy()\n",
    "all_pred_para = np.concatenate((train_para, val_para, test_para), axis=0)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(len(para_names)):\n",
    "    plt.subplot(3, 7, i + 1)\n",
    "    plt.hist(all_pred_para[:, i], bins=50, alpha=0.7, color='green')\n",
    "    plt.title(para_names[i])\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Processes Understanding\n",
    "BINN-optimized parameters are used to construct matrices in the matrix form of CLM5, which can be further used for SOC simulations. In addition, these matrices can be further used for understanding the mechanisms underlying SOC simulations:  \n",
    "#####  \n",
    "**Carbon Transfer Efficiency**: the weighted average ratio of decomposed carbon being transferred from one carbon pool to another relative to the total carbon decomposition  \n",
    "**Baseline Decomposition**: the substrate decomposability of each soil pool  \n",
    "**Environmental Modifier**: how environment (temperature, moisture, oxygen, and nitrogen) will affect the substrate decomposability  \n",
    "**Carbon Input Allocation**: how plant carbon being allocated to different soil depths  \n",
    "**Vertical Transport Rate**: how soil organic carbon moves along the soil depths  \n",
    "**Plant Carbon Input**: plant carbon input into soil  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk prediction with grid data\n",
    "with torch.no_grad():\n",
    "    temp_test_soc, temp_test_para = best_guess_model(test_x.to(device), test_z.to(device), whether_predict = 1)\n",
    "    grid_simu_soc, grid_pred_para = best_guess_model(torch.tensor(predict_data_x, dtype=torch.float32, device=device), torch.tensor(predict_data_z, dtype=torch.float32, device=device), whether_predict = 1)\n",
    "    \n",
    "carbon_input_pred, cpool_steady_state_pred, cpools_layer_pred, soc_layer_pred, total_res_time_pred, \\\n",
    "    total_res_time_base_pred, res_time_base_pools_pred, t_scaler_pred, bulk_A_pred, \\\n",
    "    w_scaler_pred, bulk_K_pred, bulk_V_pred, bulk_xi_pred, bulk_I_pred, litter_fraction_pred = fun_bulk_simu(grid_pred_para.to(device), \\\n",
    "                                                                                                    torch.tensor(predict_data_x, dtype=torch.float32, device=device))\n",
    "\n",
    "# Bulk processes for plotting\n",
    "bulk_processes = {\n",
    "    'Carbon Transfer Efficiency': bulk_A_pred,\n",
    "    'Baseline Decomposition': bulk_K_pred,\n",
    "    'Environmental Modifier': bulk_xi_pred,\n",
    "    'Carbon Input Allocation': bulk_I_pred,\n",
    "    'Vertical Transport Rate': bulk_V_pred,\n",
    "    'Plant Carbon Input': carbon_input_pred,\n",
    "}\n",
    "\n",
    "# Plot the map of the bulk processes using grid_lon and grid_lat\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "axes = axes.flatten()\n",
    "for ax, (process_name, process_data) in zip(axes, bulk_processes.items()):\n",
    "    ax.set_global()\n",
    "    ax.add_feature(cfeature.LAND, facecolor='0.95', zorder=0)\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor='white', zorder=0)\n",
    "    ax.coastlines(linewidth=0.8)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.6)\n",
    "    # Plot the data\n",
    "    sc = ax.scatter(grid_lon, grid_lat, c=process_data.cpu().numpy(), s=50, marker='s', cmap='viridis', alpha=0.8, transform=ccrs.PlateCarree())\n",
    "    margin = 1\n",
    "    ax.set_extent([lons.min()-margin, lons.max()+margin,\n",
    "                lats.min()-margin, lats.max()+margin],\n",
    "                crs=ccrs.PlateCarree())\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.4)\n",
    "    gl.right_labels = False\n",
    "    gl.top_labels = False\n",
    "    plt.colorbar(sc, ax=ax, orientation='vertical', label=process_name)\n",
    "    ax.set_title(process_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
